## å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒªã‚¹ãƒˆ chat.py
# streamlit@1.25.0
# openai@0.27.8
# langchain@0.0.266
# llama-index@0.8.5
# pypdf@3.15.2
# nltk@3.8.1
# pydantic@1.10.12
import os
import tempfile
import shutil
import json
import streamlit as st
from langchain.chat_models import ChatOpenAI

# from llama_index import VectorStoreIndex, StorageContext, ServiceContext
from llama_index import (
    LLMPredictor,
    #    PromptTemplate,
    ServiceContext,
    StorageContext,
    load_index_from_storage,
)
from llama_index.indices.base import BaseIndex
import hmac  # ãƒ­ã‚°ã‚¤ãƒ³æ©Ÿèƒ½ã«å¿…è¦
from japanese_pages import titles


# ã‚¿ã‚¤ãƒˆãƒ«
st.set_page_config(page_title="ãƒãƒ£ãƒƒãƒˆ", page_icon="ğŸ’¬")
st.markdown(
    "<h1 class='jp-san-serif'>ãƒãƒ£ãƒƒãƒˆãƒšãƒ¼ã‚¸</h1>",
    unsafe_allow_html=True,
)
titles()

st.write(
    '<span style="color:blue;">â—‹â—‹â—‹â—‹ï¼ˆä¾‹ï¼šç¤¾å†…è¦å‰‡ï¼‰ã«ã¤ã„ã¦ä½•ã§ã‚‚èã„ã¦ãã ã•ã„...ğŸ˜‰</span>',
    unsafe_allow_html=True,
)


##################################### ã‚¿ã‚¤ãƒˆãƒ«ã®CSSã‚’è‰¯ã—ãªã«è¨­å®š ############################################
# Google Fontsã‹ã‚‰Noto Sans JPãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
st.markdown(
    """
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP&display=swap" rel="stylesheet">
    """,
    unsafe_allow_html=True,
)

# Robotoãƒ•ã‚©ãƒ³ãƒˆã‚’ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—ã«ä½¿ç”¨ã™ã‚‹ãŸã‚ã®HTMLã‚¹ã‚¿ã‚¤ãƒ«
st.markdown(
    """
    <style>
    .jp-san-serif {
        font-family: 'Noto Sans JP', sans-serif;
        font-size: 1.5rem;
    }
    </style>
    """,
    unsafe_allow_html=True,
)
##################################### ã‚¿ã‚¤ãƒˆãƒ«ã®CSSã‚’è‰¯ã—ãªã«è¨­å®šï½ã“ã“ã¾ã§ ############################################


############# admin.pyã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸindex.jsoné…ä¸‹ã‚’èª­ã¿è¾¼ã‚€æ©Ÿèƒ½ ##################
if "index" not in st.session_state:
    with tempfile.TemporaryDirectory() as temp_dir:
        # index.jsonãŒã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        app_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        index_dir = os.path.join(app_root, "index.json")

        # st.write("index_dir...", index_dir)
        index_files = [
            os.path.join(index_dir, "docstore.json"),
            os.path.join(index_dir, "index_store.json"),
            os.path.join(index_dir, "vector_store.json"),
        ]

        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ†ãƒ³ãƒãƒ©ãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
        for file in index_files:
            shutil.copy(file, temp_dir)

        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo", temperature=0
        )  # llama_indexã¯LanngChainã«ä¾å­˜ã—ã¦ãŠã‚Šlangchainã®ChatOpenAIã‚¯ãƒ©ã‚¹ã‚’ä½¿ã£ã¦gpt-3.5-turboã‚’ä½¿ã†æº–å‚™ã‚’ã™ã‚‹ï¼
        # service_context = ServiceContext.from_defaults(llm=llm)
        service_context = ServiceContext.from_defaults(
            llm_predictor=LLMPredictor(
                llm=ChatOpenAI(
                    temperature=0, model_name="gpt-3.5-turbo-0613", max_tokens=512
                )
            )
        )
        storage_context = StorageContext.from_defaults(persist_dir=temp_dir)
        # index = VectorStoreIndex.load_from_disk(
        #     temp_dir, service_context=service_context
        # )
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        with open(os.path.join(temp_dir, "index_store.json"), "rb") as f:
            index_bytes = f.read()
        # index = VectorStoreIndex.load_from_bytes(
        #     index_bytes, service_context=service_context
        # )
        index = load_index_from_storage(
            storage_context=storage_context, service_context=service_context
        )

        st.session_state["index"] = index

index = st.session_state["index"]
############# admin.pyã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸindex.jsoné…ä¸‹ã‚’èª­ã¿è¾¼ã‚€æ©Ÿèƒ½ï½ã“ã“ã¾ã§ ##################

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã®é ˜åŸŸ
# user_input = st.text_input(
#     "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", key="input", placeholder="ã“ã“ã«å…¥åŠ›..."
# )
user_input = st.chat_input(
    "...è³ªå•ã‚’å…¥åŠ›ãã ã•ã„"
)  # st.text_inputã ã¨ä¸Šè¨˜ã®ã‚ˆã†ã«labelãªã©è‰²ã€…è¨­å®šã§ãã‚‹ä»£ã‚ã‚Šã«ä¸­å¤®ã«è¡¨ç¤ºã•ã‚Œã¦ã—ã¾ã†ï¼
if user_input:
    with st.spinner("è€ƒãˆä¸­..."):
        query_engine = index.as_query_engine()
        query = "\n".join(
            [f"Human: {msg}" for msg in st.session_state["chat_history"][-10:]]
            + [f"Human: {user_input}"]
        )
        answer = query_engine.query(query)
        st.session_state["chat_history"].append(user_input)
        st.session_state["chat_history"].append(answer.response)

# å¯¾è©±å±¥æ­´ã®è¡¨ç¤º
# session_state["chat_history"]ã®é…åˆ—ã®å¥‡æ•°ç•ªç›®ã®ã‚‚ã®ãªã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€å¶æ•°ç•ªç›®ãªã‚‰ChatGPTã®å›ç­”ã€ã¨ã—ã¦è¡¨ç¤º
for i, message in enumerate(st.session_state["chat_history"]):
    if i % 2 == 0:
        st.markdown(f"**ã‚ãªãŸ:** {message}")
    else:
        st.markdown(f"**ChatGPT:** {message}")
